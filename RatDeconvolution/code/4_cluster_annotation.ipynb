{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=\"C:/github/2023/RatDeconvolution\"\n",
    "path_package=\"C:/github/enan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of rat gene ontology data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filein = f\"{root}/data/rgd.gaf\" # from the web site: www.geneontology.org\n",
    "datafile = f'{root}/data/220801_go.owl' # from the web site: www.geneontology.org\n",
    "depth=5 # gene ontology depth for enrichment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import owlready2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OntologyStructure:\n",
    "    def __init__(self, datafile, root_term:str):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "        datafile : str, the url of ontology file\n",
    "        \"\"\"\n",
    "        \n",
    "        # load datafile\n",
    "        self.onto = owlready2.get_ontology(datafile).load() # load ontology file (DL from https://www.ebi.ac.uk/efo/)\n",
    "        print(datafile,\"was loaded.\")\n",
    "        \n",
    "        # determine the roots\n",
    "        roots = []\n",
    "        for efo in self.onto.classes(): # EFO0000001 is the root\n",
    "            if root_term in str(efo):\n",
    "                roots.append(efo)\n",
    "        if len(roots) == 0:\n",
    "            print('the designated root term couldnt be found')\n",
    "            for efo in self.onto.classed():\n",
    "                if 'EFO0000001' in str(efo):\n",
    "                    roots.append(efo)\n",
    "\n",
    "        # select the first root CAUTION : this must be changed if you use an ontology structure which have multiple roots\n",
    "        self.root = roots[0] \n",
    "        print(self.root, \"was determined as root.\")\n",
    "        # with BFS, determine the depth of each GO\n",
    "        self.min_d = self.__BFS() # 幅優先探索をやってる\n",
    "        print(\"Graph structure was obtained.\")\n",
    "        \n",
    "        self.string_hash = self.__create_string_hash()\n",
    "        print(\"String hashtable was created.\")\n",
    " \n",
    "    def __BFS(self):\n",
    "        \"\"\"\n",
    "        breadth-first search function\n",
    "        \n",
    "        the function to determine the depth of each term\n",
    "        \"\"\"\n",
    "        Q = deque([self.root])\n",
    "        inf_value = lambda : float('inf')\n",
    "        min_d = defaultdict(inf_value) # defaultdictでエラーしたらinfを返すようにしている. つまり届かないところ\n",
    "        min_d[self.root] = 0 # rootの深さは0と定義\n",
    "        while len(Q)>0:\n",
    "            v = Q.pop()\n",
    "            for v1 in self.onto.get_children_of(v): # vの子v1をループ\n",
    "                if min_d[v1]>10**27: # infより大きい場合. Noneでもいいような. 要するにまだmin_dが定義されていないvの子供が現れた時の判定\n",
    "                    min_d[v1] = min_d[v] + 1 # min_dがない子供が現れたらvの深さに1段階加えて更新する\n",
    "                    Q.appendleft(v1) # Qは探索すべきものなので, 新たな階層の探索のために追加する\n",
    "        return min_d # 木構造を返す: タームを入れるとその深さを返すもの\n",
    "\n",
    "    def __create_string_hash(self):\n",
    "        \"\"\"\n",
    "        string hash function\n",
    "        \n",
    "        create the hashtable to get the class instance corresponding to the given string\n",
    "        \"\"\"\n",
    "        \n",
    "        string_hash = defaultdict(str)\n",
    "        for key in self.min_d:\n",
    "            string_hash[str(key)] = key\n",
    "        return string_hash\n",
    "\n",
    "    def get_num(self, depth):\n",
    "        \"\"\"\n",
    "        getting the number of ontology term in a specific level\n",
    "        depth : int\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for key in self.min_d:\n",
    "            if self.min_d[key]==depth:\n",
    "                count+=1\n",
    "        return count\n",
    "    \n",
    "    def get_term(self, depth):\n",
    "        \"\"\"\n",
    "        getting the term in the specific level\n",
    "        depth :  int\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        term = []\n",
    "        for key in self.min_d:\n",
    "            if self.min_d[key]==depth:\n",
    "                res.append(str(key))\n",
    "                term.append(str(key.label[0]))\n",
    "        return res,term\n",
    "    \n",
    "    def get_downstream_ID(self):\n",
    "        max_depth = max(list(self.min_d.values()))\n",
    "        print('max depth :',max_depth)\n",
    "        total_res = []\n",
    "        total_term = []\n",
    "        for i in range(1,max_depth+1):\n",
    "            tmp_res = []\n",
    "            tmp_term = []\n",
    "            for key in self.min_d:\n",
    "                if self.min_d[key]==i:\n",
    "                    tmp_res.append(str(key))\n",
    "                    tmp_term.append(str(key.label[0]))\n",
    "            \n",
    "            for t in tmp_res:\n",
    "                total_res.append(t)\n",
    "            for s in tmp_term:\n",
    "                total_term.append(s)\n",
    "        return total_res, total_term\n",
    "    \n",
    "    def get_upstream(self, go, depth):\n",
    "        \"\"\"\n",
    "        getting the upstream GO in a specific level\n",
    "        \n",
    "        go : GO class instance\n",
    "        depth : int\n",
    "        \"\"\"\n",
    "        if self.min_d[go]<2:\n",
    "            return set()\n",
    "        if self.min_d[go]==2:\n",
    "            return set([go])\n",
    "        \n",
    "        up = set()\n",
    "\n",
    "        Q = deque([go])\n",
    "        while len(Q)>0:\n",
    "            v = Q.pop()\n",
    "            try:\n",
    "                for v1 in self.onto.get_parents_of(v):\n",
    "                    if self.min_d[v1]==depth:\n",
    "                        up.add(v1)\n",
    "                    else:\n",
    "                        Q.appendleft(v1)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        return up\n",
    "    \n",
    "    def str2go(self, string):\n",
    "        \"\"\"\n",
    "        str to GO class instance\n",
    "        \"\"\"\n",
    "        return self.string_hash[string]\n",
    "\n",
    "    def go2str(self, go):\n",
    "        \"\"\"\n",
    "        GO class instance to str\n",
    "        \"\"\"\n",
    "        return str(go)\n",
    "\n",
    "    def get_depth(self, go):\n",
    "        \"\"\"\n",
    "        get the depth of GO\n",
    "        \"\"\"\n",
    "        return self.min_d[go]\n",
    "        \n",
    "    def get_term(self, depth):\n",
    "        \"\"\"\n",
    "        getting the term in the specific level\n",
    "        depth :  int\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        term = []\n",
    "        for key in self.min_d:\n",
    "            if self.min_d[key]==depth:\n",
    "                res.append(str(key))\n",
    "                term.append(str(key.label[0]))\n",
    "        return res,term\n",
    "    \n",
    "    def get_downstream(self, go):\n",
    "        \"\"\"\n",
    "        getting the deepest downstream go terms\n",
    "        go : GO class instancce\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        Q = deque(self.onto.get_children_of(go))\n",
    "        if len(Q)==0:\n",
    "            return [go]\n",
    "        \n",
    "        while len(Q)>0:\n",
    "            v = Q.pop()\n",
    "            res.append(v)\n",
    "            v_child = self.onto.get_children_of(v)\n",
    "            if len(v_child)==0:\n",
    "                pass\n",
    "            else:\n",
    "                for v1 in v_child:\n",
    "                    Q.appendleft(v1)\n",
    "        res = list(set(res))\n",
    "        return res     \n",
    "\n",
    "def load_go(gene_dict=dict()):\n",
    "    terms = []\n",
    "    members = []\n",
    "    for key in gene_dict.keys():\n",
    "        terms.append(key)\n",
    "        members.append(gene_dict[key])\n",
    "    return terms, members\n",
    "\n",
    "def create_dict(datafile='', gene_dict=dict(), depth:int=4):\n",
    "    # load go.owl file\n",
    "    dat = OntologyStructure(datafile, 'obo.GO_0008150') #GO_0008150 : Biological Process\n",
    "    res, term = dat.get_term(depth)\n",
    "    print('Indicated depth GO terms : {}'.format(len(res)))\n",
    "    corresp_res = [dat.get_downstream(dat.str2go(i)) for i in tqdm(res)]\n",
    "    corresp_res = [[str(v).replace('obo.GO_', 'GO:') for v in i] for i in corresp_res]\n",
    "\n",
    "    # load gene file\n",
    "    terms, members = load_go(gene_dict=gene_dict)\n",
    "    #terms = [i.split('(')[1].split(')')[0] for i in terms]\n",
    "    go_dict = dict(zip(terms, members))\n",
    "    \n",
    "    # create dict\n",
    "    res_dict=dict()\n",
    "    goperterm=[]\n",
    "    for root, i, t in zip(res,corresp_res, term):\n",
    "        res_temp = set()\n",
    "        for v in i:\n",
    "            genes_temp=go_dict.get(v, 'no')\n",
    "            if genes_temp=='no':\n",
    "                pass\n",
    "            else:\n",
    "                res_temp = res_temp or genes_temp\n",
    "        if len(res_temp)>0:\n",
    "            res_dict[root.replace('obo.GO_','GO:')+'_'+t]=res_temp\n",
    "            goperterm.append(len(res_temp))\n",
    "\n",
    "    print('No. of GO terms : {}'.format(len(res_dict)))\n",
    "    print('No. of genes / term : {}'.format(np.mean(goperterm)))\n",
    "\n",
    "    return res_dict\n",
    "\n",
    "def extract(sentence):\n",
    "    res = []\n",
    "    temp1 = sentence.split(\"RGD\\t\")\n",
    "    flag=False\n",
    "    for i in temp1:\n",
    "        temp2 = i.split(\"\\t\")\n",
    "        if len(temp2)>5:\n",
    "            symbol = temp2[1]\n",
    "            term = temp2[3]\n",
    "            if \"GO:\" in term:\n",
    "                res.append([symbol, term])\n",
    "                flag=True\n",
    "    temp1 = sentence.split(\"UniProtKB\\t\")\n",
    "    for i in temp1:\n",
    "        temp2 = i.split(\"\\t\")\n",
    "        if len(temp2)>5:\n",
    "            symbol = temp2[1]\n",
    "            term = temp2[3]\n",
    "            if \"GO:\" in term:\n",
    "                res.append([symbol, term])\n",
    "                flag=True\n",
    "    if flag:\n",
    "        return res, []\n",
    "    else:\n",
    "        return res, [sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filein, encoding='utf-8', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    res = [cols for cols in reader]\n",
    "\n",
    "res = res[39:]\n",
    "res_open=[]\n",
    "res_error1=[]\n",
    "res_error2=[]\n",
    "for i in res:\n",
    "    if type(i)==str:\n",
    "        res_temp, error = extract(i)\n",
    "        res_open += res_temp\n",
    "        res_error1 += error\n",
    "    elif len(i)==1:\n",
    "        res_temp, error = extract(i[0])\n",
    "        res_open += res_temp\n",
    "        res_error1 += error\n",
    "    else:\n",
    "        for v in i:\n",
    "            if type(v)==str:\n",
    "                res_temp, error = extract(v)\n",
    "                res_open += res_temp\n",
    "                res_error2 += error\n",
    "            else:\n",
    "                res_temp, error = extract(v[0])\n",
    "                res_open += res_temp\n",
    "                res_error2 += error\n",
    "e2_go = []\n",
    "for i in res_error2:\n",
    "    if \"GO\" in i:\n",
    "        e2_go.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['regulates(GO:0006366)\\t',\n",
       " '8-sialyltransferase 5\\t\\tgene\\ttaxon:10116\\t20220421\\tGOC\\t\\t',\n",
       " '8-sialyltransferase 5\\t\\tgene\\ttaxon:10116\\t20220421\\tGOC\\t\\t',\n",
       " ' ankyrin repeat and coiled-coil containing 1\\t\\tgene\\ttaxon:10116\\t20180711\\tSynGO\\tpart_of(UBERON:0002421)\\t',\n",
       " ' ankyrin repeat and coiled-coil containing 1\\t\\tgene\\ttaxon:10116\\t20180711\\tSynGO\\tpart_of(UBERON:0002421)\\t',\n",
       " ' ankyrin repeat and coiled-coil containing 1\\t\\tgene\\ttaxon:10116\\t20180711\\tSynGO\\tpart_of(GO:0098978)',\n",
       " ' ankyrin repeat and coiled-coil containing 1\\t\\tgene\\ttaxon:10116\\t20180711\\tSynGO\\tpart_of(GO:0098978)',\n",
       " ' ankyrin repeat and coiled-coil containing 1\\t\\tgene\\ttaxon:10116\\t20180711\\tSynGO\\toccurs_in(GO:0098978)',\n",
       " ' ankyrin repeat and coiled-coil containing 1\\t\\tgene\\ttaxon:10116\\t20180711\\tSynGO\\toccurs_in(GO:0098978)',\n",
       " ' member 2\\t\\tgene\\ttaxon:10116\\t20050726\\tRGD\\tnegatively_regulates(GO:0008283)\\t']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2_go[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = dict()\n",
    "term_set = set()\n",
    "for gene, term in res_open:\n",
    "    if not term in term_set:\n",
    "        term_set.add(term)\n",
    "        res_dict[term]=set([gene])\n",
    "    else:\n",
    "        res_dict[term].add(gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/github/2023/RatDeconvolution/data/220801_go.owl was loaded.\n",
      "obo.GO_0008150 was determined as root.\n",
      "Graph structure was obtained.\n",
      "String hashtable was created.\n",
      "Indicated depth GO terms : 6712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6712/6712 [00:00<00:00, 8258.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of GO terms : 3447\n",
      "No. of genes / term : 9.931534667827096\n"
     ]
    }
   ],
   "source": [
    "res = create_dict(datafile=datafile, gene_dict=res_dict, depth=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(res, f\"{root}/result/go_rat_depth5.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ssGSEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import codecs\n",
    "\n",
    "sys.path.append(\"C:/github/enan\")\n",
    "from enan import binom, ssgsea\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_sample(df, df_b):\n",
    "    temp = df_b.loc[df.columns.tolist()]\n",
    "    name = temp[\"COMPOUND_NAME\"].tolist()\n",
    "    dose = temp[\"DOSE_LEVEL\"].tolist()\n",
    "    time = temp[\"SACRIFICE_PERIOD\"].tolist()\n",
    "    ind = [f\"{i}_{j}_{k}\" for i, j, k in zip(name, dose, time)]\n",
    "    return ind\n",
    "\n",
    "def load_transcriptome(target_compounds):\n",
    "    # load transcriptome\n",
    "    df_target = pd.read_csv(f\"{root}/data/tggate_transcriptome.csv\",index_col=0)\n",
    "    df_sample = pd.read_csv(f\"{root}/data/tggate_sample_information.csv\",index_col=0)\n",
    "    df_target.columns=[str(i) for i in df_target.columns]\n",
    "    df_sample.index=[str(i) for i in df_sample.index]\n",
    "    df_target.columns = annotation_sample(df_target, df_sample)\n",
    "    target_lst = [\n",
    "        f'{compound}_{conc}_{time}'\n",
    "        for compound in target_compounds \n",
    "        for conc in [\"High\", \"Control\"]\n",
    "        for time in [\"3 hr\", \"6 hr\", \"9 hr\", \"24 hr\"]\n",
    "    ]\n",
    "    df_target=df_target.loc[:,target_lst]\n",
    "    return df_target\n",
    "\n",
    "def calc_ssGSEA(df, depth=5, limit=10):\n",
    "    # load\n",
    "    set_depth = pd.read_pickle(f\"{root}/result/go_rat_depth{str(depth)}.pickle\")\n",
    "    ref = dict()\n",
    "    set_whole=set()\n",
    "    for i in set_depth.keys():\n",
    "        temp = set_depth[i]\n",
    "        if len(temp)>limit-1:\n",
    "            ref[i]=temp\n",
    "            set_whole = set_whole|set_depth[i]\n",
    "    print(f\"ref length: {len(ref)}\")\n",
    "    print(f\"whole genes: {len(set_whole)}\")\n",
    "\n",
    "    dat = ssgsea.ssGSEA()\n",
    "    dat.fit(ref)\n",
    "    dat.set_whole(set_whole)\n",
    "    res = dat.calc(df, method=\"kuiper\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_compounds=[\n",
    "    \"naphthyl isothiocyanate\",\"bromobenzene\",\"simvastatin\",\"enalapril\",\n",
    "    \"gefitinib\",\"metformin\",\"tiopronin\",\"colchicine\",\n",
    "    \"bortezomib\",\"methylene dianiline\",\"galactosamine\",\"thioacetamide\",\n",
    "    \"LPS\",\"cycloheximide\",\"tacrine\",\"nitrofurazone\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_transcriptome(target_compounds)\n",
    "col = df.columns.tolist()\n",
    "df.columns=range(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref length: 600\n",
      "whole genes: 8989\n",
      "Kuiper method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 383/383 [06:43<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "res = calc_ssGSEA(df, depth=5)\n",
    "res.columns=col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv(f\"{root}/result/res_depth5_kuiper.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis/Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "immune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
